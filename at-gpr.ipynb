{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg\n",
    "from scipy.optimize import minimize\n",
    "from scipy.io import loadmat # For SARCOS\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- Kernel Function ---\n",
    "class SquaredExponentialKernel:\n",
    "    def __init__(self, length_scale=1.0, sigma_f=1.0):\n",
    "        self.length_scale = length_scale\n",
    "        self.sigma_f = sigma_f\n",
    "\n",
    "    def get_params(self):\n",
    "        return np.array([self.length_scale, self.sigma_f])\n",
    "\n",
    "    def set_params(self, params):\n",
    "        self.length_scale = params[0]\n",
    "        self.sigma_f = params[1]\n",
    "\n",
    "    def __call__(self, X1, X2):\n",
    "        if X1.ndim == 1: X1 = X1[:, np.newaxis]\n",
    "        if X2.ndim == 1: X2 = X2[:, np.newaxis]\n",
    "\n",
    "        if X1.shape[1] != X2.shape[1]:\n",
    "            raise ValueError(f\"X1 and X2 must have the same number of features. Got {X1.shape[1]} and {X2.shape[1]}\")\n",
    "\n",
    "        sqdist = np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * np.dot(X1, X2.T)\n",
    "        sqdist = np.clip(sqdist, 0, np.inf) # Ensure non-negative for sqrt if used elsewhere\n",
    "\n",
    "        return self.sigma_f**2 * np.exp(-0.5 / self.length_scale**2 * sqdist)\n",
    "\n",
    "# Helper to compute covariance matrix\n",
    "def calculate_covariance_matrix(X1, X2, kernel_func):\n",
    "    return kernel_func(X1, X2)\n",
    "\n",
    "# --- NMSE Metric ---\n",
    "def nmse(y_true, y_pred):\n",
    "    y_true_v = np.asarray(y_true).flatten()\n",
    "    y_pred_v = np.asarray(y_pred).flatten()\n",
    "    if len(y_true_v) == 0 or len(y_pred_v) == 0: return np.nan\n",
    "    mse = np.mean((y_true_v - y_pred_v)**2)\n",
    "    var_true = np.var(y_true_v)\n",
    "    if var_true < 1e-9: # Avoid division by zero if true values are constant\n",
    "        return mse if mse > 1e-9 else 0.0\n",
    "    return mse / var_true\n",
    "\n",
    "# --- AT-GP Model (from paper) ---\n",
    "class ATGP:\n",
    "    def __init__(self, X_S, y_S, X_T, y_T,\n",
    "                 initial_length_scale=1.0, initial_sigma_f=1.0,\n",
    "                 initial_b=1.0, initial_mu=1.0,\n",
    "                 initial_noise_S_std=0.1, initial_noise_T_std=0.1):\n",
    "        self.X_S = X_S\n",
    "        self.y_S = y_S.reshape(-1, 1)\n",
    "        self.X_T = X_T\n",
    "        self.y_T = y_T.reshape(-1, 1)\n",
    "\n",
    "        self._length_scale = initial_length_scale\n",
    "        self._sigma_f = initial_sigma_f\n",
    "        self._b = initial_b\n",
    "        self._mu = initial_mu\n",
    "        self._noise_S_std = initial_noise_S_std\n",
    "        self._noise_T_std = initial_noise_T_std\n",
    "\n",
    "        self.base_kernel = SquaredExponentialKernel(self._length_scale, self._sigma_f)\n",
    "        self.fitted = False\n",
    "        self.optimized_params_ = {}\n",
    "\n",
    "    def _get_params_array(self):\n",
    "        return np.array([self._length_scale, self._sigma_f, self._b, self._mu,\n",
    "                         self._noise_S_std, self._noise_T_std])\n",
    "\n",
    "    def _set_params_from_array(self, params_array):\n",
    "        self._length_scale = params_array[0]\n",
    "        self._sigma_f = params_array[1]\n",
    "        self.base_kernel.set_params(params_array[:2])\n",
    "        self._b = params_array[2]\n",
    "        self._mu = params_array[3]\n",
    "        self._noise_S_std = params_array[4]\n",
    "        self._noise_T_std = params_array[5]\n",
    "\n",
    "    def _get_lambda(self, b_param=None, mu_param=None):\n",
    "        b_to_use = b_param if b_param is not None else self._b\n",
    "        mu_to_use = mu_param if mu_param is not None else self._mu\n",
    "\n",
    "        # Parameter constraints from practical application (and to avoid math errors)\n",
    "        b_to_use = max(1e-5, b_to_use)\n",
    "        mu_to_use = max(1e-5, mu_to_use) # mu > -1, but for (1+mu) often > 0 is better\n",
    "\n",
    "        term_base = 1.0 / (1.0 + mu_to_use)\n",
    "        lambda_val = -1.0\n",
    "        if term_base < 0 : # (1+mu) is negative\n",
    "             lambda_val = -1.0 # Or some other defined behavior\n",
    "        elif abs(term_base) > 1e6 and b_to_use > 1 and term_base > 0: # Avoid large number to large power\n",
    "            lambda_val = 1.0\n",
    "        else:\n",
    "            try:\n",
    "                pow_term = np.power(term_base, b_to_use)\n",
    "                lambda_val = 2 * pow_term - 1\n",
    "            except (OverflowError, ValueError):\n",
    "                lambda_val = 1.0 if term_base > 1 else -1.0 # Fallback for extreme values\n",
    "\n",
    "        return np.clip(lambda_val, -1.0, 1.0)\n",
    "\n",
    "\n",
    "    def log_marginal_likelihood_conditional(self, params_array_opt):\n",
    "        # Ensure parameters are positive where necessary\n",
    "        params_array_opt = np.maximum(params_array_opt, 1e-5)\n",
    "\n",
    "        current_length_scale = params_array_opt[0]\n",
    "        current_sigma_f = params_array_opt[1]\n",
    "        temp_kernel = SquaredExponentialKernel(current_length_scale, current_sigma_f)\n",
    "\n",
    "        current_b = params_array_opt[2]\n",
    "        current_mu = params_array_opt[3]\n",
    "        temp_lambda = self._get_lambda(b_param=current_b, mu_param=current_mu)\n",
    "\n",
    "        current_noise_S_var = params_array_opt[4]**2\n",
    "        current_noise_T_var = params_array_opt[5]**2\n",
    "\n",
    "        jitter = 1e-7 # For numerical stability\n",
    "\n",
    "        K11 = calculate_covariance_matrix(self.X_S, self.X_S, temp_kernel)\n",
    "        K22 = calculate_covariance_matrix(self.X_T, self.X_T, temp_kernel)\n",
    "        K_TS_base = calculate_covariance_matrix(self.X_T, self.X_S, temp_kernel)\n",
    "\n",
    "        K21 = temp_lambda * K_TS_base\n",
    "        K12 = K21.T # Or temp_lambda * K_TS_base.T\n",
    "\n",
    "        try:\n",
    "            K11_noisy = K11 + current_noise_S_var * np.eye(K11.shape[0]) + jitter * np.eye(K11.shape[0])\n",
    "            L_K11_noisy = np.linalg.cholesky(K11_noisy)\n",
    "\n",
    "            # inv(K11_noisy) @ y_S\n",
    "            K11_noisy_inv_yS = scipy.linalg.solve_triangular(L_K11_noisy.T, scipy.linalg.solve_triangular(L_K11_noisy, self.y_S, lower=True, check_finite=False), lower=False, check_finite=False)\n",
    "            mu_t = K21 @ K11_noisy_inv_yS\n",
    "\n",
    "            # inv(K11_noisy) @ K12\n",
    "            K11_noisy_inv_K12 = scipy.linalg.solve_triangular(L_K11_noisy.T, scipy.linalg.solve_triangular(L_K11_noisy, K12, lower=True, check_finite=False), lower=False, check_finite=False)\n",
    "\n",
    "            C_t_main_term = K22 - K21 @ K11_noisy_inv_K12\n",
    "            C_t = C_t_main_term + current_noise_T_var * np.eye(K22.shape[0]) + jitter * np.eye(K22.shape[0])\n",
    "\n",
    "            L_Ct = np.linalg.cholesky(C_t)\n",
    "            log_det_Ct = 2 * np.sum(np.log(np.diag(L_Ct)))\n",
    "\n",
    "            y_T_minus_mu_t = self.y_T - mu_t\n",
    "            # inv(C_t) @ (y_T - mu_t)\n",
    "            Ct_inv_y_minus_mu = scipy.linalg.solve_triangular(L_Ct.T, scipy.linalg.solve_triangular(L_Ct, y_T_minus_mu_t, lower=True, check_finite=False), lower=False, check_finite=False)\n",
    "\n",
    "            term2_quadratic = y_T_minus_mu_t.T @ Ct_inv_y_minus_mu\n",
    "        except np.linalg.LinAlgError:\n",
    "            return -np.inf # Penalize if Cholesky fails\n",
    "\n",
    "        log_likelihood = -0.5 * log_det_Ct - 0.5 * term2_quadratic - len(self.y_T)/2.0 * np.log(2 * np.pi)\n",
    "\n",
    "        if np.isnan(log_likelihood) or not np.isfinite(log_likelihood):\n",
    "            return -np.inf\n",
    "        return log_likelihood.item()\n",
    "\n",
    "    def fit(self, method='L-BFGS-B', disp=False, maxiter=200):\n",
    "        initial_params = self._get_params_array()\n",
    "        if disp:\n",
    "            print(\"  Starting AT-GP optimization...\")\n",
    "            print(f\"    Initial AT-GP params (ls, sf, b, mu, noise_S, noise_T): {np.round(initial_params,3)}\")\n",
    "            print(f\"    Initial AT-GP lambda: {self._get_lambda():.3f}\")\n",
    "\n",
    "        bounds = [\n",
    "            (1e-5, 1e3),  # length_scale\n",
    "            (1e-5, 1e3),  # sigma_f\n",
    "            (1e-5, 1e2),  # b\n",
    "            (1e-5, 1e2),  # mu (mu > -1 for (1+mu) to be positive in (1/(1+mu)))\n",
    "            (1e-5, 1e2),  # noise_S_std\n",
    "            (1e-5, 1e2)   # noise_T_std\n",
    "        ]\n",
    "\n",
    "        objective = lambda params_opt: -self.log_marginal_likelihood_conditional(params_opt)\n",
    "\n",
    "        result = minimize(objective, initial_params, method=method, bounds=bounds, options={'disp': disp, 'maxiter': maxiter, 'ftol': 1e-7, 'gtol': 1e-5})\n",
    "\n",
    "        if disp:\n",
    "            print(\"  AT-GP optimization finished.\")\n",
    "            print(f\"    Success: {result.success}, Message: {result.message}\")\n",
    "            print(f\"    Optimized AT-GP params (ls, sf, b, mu, noise_S, noise_T): {np.round(result.x,3)}\")\n",
    "\n",
    "\n",
    "        if result.success or \"CONVERGENCE\" in result.message.upper() :\n",
    "            self._set_params_from_array(result.x)\n",
    "            self.fitted = True\n",
    "            self.optimized_params_ = dict(zip(['ls', 'sf', 'b', 'mu', 'noise_S', 'noise_T', 'lambda'],\n",
    "                                           list(np.round(result.x,4)) + [np.round(self._get_lambda(),4)]))\n",
    "            if disp:\n",
    "                 print(f\"    Optimized AT-GP lambda: {self._get_lambda():.3f}\")\n",
    "        else:\n",
    "            if disp:\n",
    "                print(f\"    Warning: AT-GP Optimization may not have fully converged: {result.message}\")\n",
    "            # Still set params to the best found\n",
    "            self._set_params_from_array(result.x)\n",
    "            self.fitted = True\n",
    "            self.optimized_params_ = dict(zip(['ls', 'sf', 'b', 'mu', 'noise_S', 'noise_T', 'lambda'],\n",
    "                                           list(np.round(result.x,4)) + [np.round(self._get_lambda(),4)]))\n",
    "            if disp:\n",
    "                 print(f\"    Resulting AT-GP lambda (from potentially non-converged params): {self._get_lambda():.3f}\")\n",
    "        return result\n",
    "\n",
    "    def predict(self, X_star_T):\n",
    "        if not self.fitted :\n",
    "             print(\"Warning: ATGP model is not fitted. Predictions based on initial parameters.\")\n",
    "\n",
    "        lambda_val = self._get_lambda()\n",
    "        noise_S_var_val = self._noise_S_std**2\n",
    "        noise_T_var_val = self._noise_T_std**2\n",
    "        jitter = 1e-7\n",
    "\n",
    "        N_S = self.X_S.shape[0]\n",
    "        N_T = self.X_T.shape[0]\n",
    "        N_all = N_S + N_T\n",
    "\n",
    "        # Construct the full data matrices for the combined system (Eq 7 context)\n",
    "        # X_all = np.vstack((self.X_S, self.X_T)) # Not explicitly needed for K_tilde parts\n",
    "        y_all = np.vstack((self.y_S, self.y_T))\n",
    "\n",
    "        K_SS = calculate_covariance_matrix(self.X_S, self.X_S, self.base_kernel)\n",
    "        K_TT = calculate_covariance_matrix(self.X_T, self.X_T, self.base_kernel)\n",
    "        K_ST_base = calculate_covariance_matrix(self.X_S, self.X_T, self.base_kernel) # K_S_TargTrain\n",
    "\n",
    "        K_tilde = np.zeros((N_all, N_all))\n",
    "        K_tilde[:N_S, :N_S] = K_SS\n",
    "        K_tilde[N_S:, N_S:] = K_TT\n",
    "        K_tilde[:N_S, N_S:] = lambda_val * K_ST_base\n",
    "        K_tilde[N_S:, :N_S] = lambda_val * K_ST_base.T\n",
    "\n",
    "        Big_Lambda_diag = np.concatenate([np.full(N_S, noise_S_var_val),\n",
    "                                          np.full(N_T, noise_T_var_val)])\n",
    "        Big_Lambda = np.diag(Big_Lambda_diag)\n",
    "\n",
    "        C_tilde = K_tilde + Big_Lambda + jitter * np.eye(N_all) # This is C in Eq 7\n",
    "        L_Ctilde = None\n",
    "        try:\n",
    "            L_Ctilde = np.linalg.cholesky(C_tilde)\n",
    "            # alpha = C_tilde_inv @ y_all\n",
    "            alpha = scipy.linalg.solve_triangular(L_Ctilde.T, scipy.linalg.solve_triangular(L_Ctilde, y_all, lower=True, check_finite=False), lower=False, check_finite=False)\n",
    "        except np.linalg.LinAlgError:\n",
    "            # Fallback to pseudo-inverse if Cholesky fails\n",
    "            C_tilde_inv = np.linalg.pinv(C_tilde)\n",
    "            alpha = C_tilde_inv @ y_all\n",
    "\n",
    "        # k_x in Eq 7 (covariance between test point X_star_T and all training data)\n",
    "        k_star_S_base = calculate_covariance_matrix(X_star_T, self.X_S, self.base_kernel) # K_Test_Source\n",
    "        k_star_T_base = calculate_covariance_matrix(X_star_T, self.X_T, self.base_kernel) # K_Test_TargTrain\n",
    "        # The paper implies kx has lambda for source part, 1 for target part when predicting for target\n",
    "        k_x_star_rows = np.hstack((lambda_val * k_star_S_base, k_star_T_base))\n",
    "\n",
    "        mean_star = k_x_star_rows @ alpha # m(x) = kx C̃⁻¹ y (Eq 7)\n",
    "\n",
    "        # c in Eq 7 is k(x,x) + beta_t^-1 (target noise variance)\n",
    "        # k(x,x) for test points\n",
    "        k_star_star_diag = np.diag(self.base_kernel(X_star_T, X_star_T))\n",
    "        c_diag = k_star_star_diag + noise_T_var_val # This is 'c' in sigma^2(x) = c - kx C̃⁻¹ kxᵀ\n",
    "\n",
    "        if L_Ctilde is not None:\n",
    "            # v = L_Ctilde^-1 @ k_x_star_rows.T\n",
    "            v = scipy.linalg.solve_triangular(L_Ctilde, k_x_star_rows.T, lower=True, check_finite=False)\n",
    "            var_reduction_diag = np.sum(v**2, axis=0) # diag(kx C̃⁻¹ kxᵀ)\n",
    "        else:\n",
    "             # If Cholesky failed, use the pseudo-inverse C_tilde_inv\n",
    "             var_reduction_diag = np.diag(k_x_star_rows @ C_tilde_inv @ k_x_star_rows.T)\n",
    "\n",
    "\n",
    "        variance_star_diag = c_diag - var_reduction_diag\n",
    "        variance_star_diag = np.clip(variance_star_diag, jitter, np.inf) # Ensure positive variance\n",
    "\n",
    "        return mean_star, variance_star_diag.reshape(-1,1)\n",
    "\n",
    "\n",
    "# --- Simple GPR for Baselines ---\n",
    "class SimpleGPR:\n",
    "    def __init__(self, initial_length_scale=1.0, initial_sigma_f=1.0, initial_noise_std=0.1):\n",
    "        self._length_scale = initial_length_scale\n",
    "        self._sigma_f = initial_sigma_f\n",
    "        self._noise_std = initial_noise_std\n",
    "        self.kernel = SquaredExponentialKernel(self._length_scale, self._sigma_f)\n",
    "        self.X_train, self.y_train, self.alpha_, self.L_ = None, None, None, None\n",
    "        self.fitted = False\n",
    "        self.optimized_params_ = {}\n",
    "\n",
    "\n",
    "    def _get_params_array(self):\n",
    "        return np.array([self._length_scale, self._sigma_f, self._noise_std])\n",
    "\n",
    "    def _set_params_from_array(self, params_array):\n",
    "        self._length_scale = params_array[0]\n",
    "        self._sigma_f = params_array[1]\n",
    "        self.kernel.set_params(params_array[:2])\n",
    "        self._noise_std = params_array[2]\n",
    "\n",
    "    def log_marginal_likelihood(self, params_array_opt, X, y):\n",
    "        params_array_opt = np.maximum(params_array_opt, 1e-5) # Ensure positive\n",
    "        current_length_scale, current_sigma_f = params_array_opt[0], params_array_opt[1]\n",
    "        temp_kernel = SquaredExponentialKernel(current_length_scale, current_sigma_f)\n",
    "        current_noise_var = params_array_opt[2]**2\n",
    "        jitter = 1e-7\n",
    "\n",
    "        N = X.shape[0]\n",
    "        K = calculate_covariance_matrix(X, X, temp_kernel)\n",
    "        K_noisy = K + current_noise_var * np.eye(N) + jitter * np.eye(N)\n",
    "\n",
    "        try:\n",
    "            L = np.linalg.cholesky(K_noisy)\n",
    "            # alpha_solve = inv(K_noisy) @ y\n",
    "            alpha_solve = scipy.linalg.solve_triangular(L.T, scipy.linalg.solve_triangular(L, y, lower=True, check_finite=False), lower=False, check_finite=False)\n",
    "            log_det_K_noisy = 2 * np.sum(np.log(np.diag(L)))\n",
    "        except np.linalg.LinAlgError: return -np.inf\n",
    "\n",
    "        lml = -0.5 * y.T @ alpha_solve - 0.5 * log_det_K_noisy - N/2.0 * np.log(2 * np.pi)\n",
    "        if np.isnan(lml) or not np.isfinite(lml): return -np.inf\n",
    "        return lml.item()\n",
    "\n",
    "    def fit(self, X_train, y_train, method='L-BFGS-B', disp=False, maxiter=100, model_name_for_print=\"SimpleGPR\"):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train.reshape(-1,1)\n",
    "\n",
    "        initial_params = self._get_params_array()\n",
    "        if disp:\n",
    "            print(f\"  Starting {model_name_for_print} optimization...\")\n",
    "            print(f\"    Initial params (ls, sf, noise): {np.round(initial_params,3)}\")\n",
    "\n",
    "        bounds = [(1e-5, 1e3), (1e-5, 1e3), (1e-5, 1e2)] # ls, sf, noise_std\n",
    "\n",
    "        objective = lambda params_opt: -self.log_marginal_likelihood(params_opt, self.X_train, self.y_train)\n",
    "        result = minimize(objective, initial_params, method=method, bounds=bounds, options={'disp': disp, 'maxiter': maxiter, 'ftol': 1e-7, 'gtol': 1e-5})\n",
    "\n",
    "        if disp:\n",
    "            print(f\"  {model_name_for_print} optimization finished.\")\n",
    "            print(f\"    Success: {result.success}, Message: {result.message}\")\n",
    "            print(f\"    Optimized params (ls, sf, noise): {np.round(result.x,3)}\")\n",
    "\n",
    "        self._set_params_from_array(result.x)\n",
    "        self.optimized_params_ = dict(zip(['ls', 'sf', 'noise'], np.round(result.x,4)))\n",
    "\n",
    "\n",
    "        K_final = self.kernel(self.X_train, self.X_train)\n",
    "        K_noisy_final = K_final + (self._noise_std**2) * np.eye(self.X_train.shape[0]) + 1e-7 * np.eye(self.X_train.shape[0])\n",
    "        try:\n",
    "            self.L_ = np.linalg.cholesky(K_noisy_final)\n",
    "            self.alpha_ = scipy.linalg.solve_triangular(self.L_.T, scipy.linalg.solve_triangular(self.L_, self.y_train, lower=True, check_finite=False), lower=False, check_finite=False)\n",
    "            self.fitted = True\n",
    "        except np.linalg.LinAlgError:\n",
    "            if disp:\n",
    "                print(\"    Warning: Cholesky failed in SimpleGPR fit post-optimization. Using PINV for prediction.\")\n",
    "            K_noisy_inv = np.linalg.pinv(K_noisy_final)\n",
    "            self.alpha_ = K_noisy_inv @ self.y_train\n",
    "            self.L_ = None # Indicate Cholesky failed\n",
    "            self.fitted = True\n",
    "        return result\n",
    "\n",
    "    def predict(self, X_star):\n",
    "        if not self.fitted: raise RuntimeError(\"SimpleGPR must be fitted before prediction.\")\n",
    "        jitter = 1e-9\n",
    "\n",
    "        K_star = self.kernel(X_star, self.X_train) # Covariance between test and train\n",
    "        mean_star = K_star @ self.alpha_\n",
    "\n",
    "        K_star_star_diag = np.diag(self.kernel(X_star, X_star)) # Covariance between test points themselves (diagonal)\n",
    "\n",
    "        if self.L_ is not None:\n",
    "            # v = L^-1 @ K_star.T\n",
    "            v = scipy.linalg.solve_triangular(self.L_, K_star.T, lower=True, check_finite=False)\n",
    "            var_reduction_diag = np.sum(v**2, axis=0) # diag(K_star @ K_noisy_inv @ K_star.T)\n",
    "        else:\n",
    "            # Fallback if Cholesky failed during fit\n",
    "            K_noisy = self.kernel(self.X_train, self.X_train) + (self._noise_std**2) * np.eye(self.X_train.shape[0]) + 1e-7 * np.eye(self.X_train.shape[0])\n",
    "            K_noisy_inv = np.linalg.pinv(K_noisy)\n",
    "            var_reduction_diag = np.diag(K_star @ K_noisy_inv @ K_star.T)\n",
    "\n",
    "        variance_star_diag = K_star_star_diag + (self._noise_std**2) - var_reduction_diag\n",
    "        variance_star_diag = np.clip(variance_star_diag, jitter, np.inf) # Ensure positive\n",
    "\n",
    "        return mean_star, variance_star_diag.reshape(-1,1)\n",
    "\n",
    "\n",
    "def run_wine_experiment(verbose_optimization, seed):\n",
    "    print(\"\\n--- Starting WINE Dataset Experiment ---\")\n",
    "    try:\n",
    "        wine_white_df = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv', sep=';')\n",
    "        wine_red_df = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv', sep=';')\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load wine datasets: {e}. Skipping WINE experiment.\")\n",
    "        return\n",
    "\n",
    "    X_S_raw = wine_white_df.drop('quality', axis=1).values\n",
    "    y_S_raw = wine_white_df['quality'].values\n",
    "    X_T_raw = wine_red_df.drop('quality', axis=1).values\n",
    "    y_T_raw = wine_red_df['quality'].values\n",
    "\n",
    "    scaler_S = StandardScaler()\n",
    "    X_S = scaler_S.fit_transform(X_S_raw)\n",
    "\n",
    "    scaler_T_all_wine = StandardScaler() # Specific scaler for wine target\n",
    "    X_T_scaled_all = scaler_T_all_wine.fit_transform(X_T_raw)\n",
    "\n",
    "    # Use 5% of Target data for training as per paper's setup for experiments section\n",
    "    X_T_train, X_T_test, y_T_train, y_T_test = train_test_split(\n",
    "        X_T_scaled_all, y_T_raw, train_size=0.05, random_state=seed\n",
    "    )\n",
    "\n",
    "    print(f\"Source (White Wine): X_S {X_S.shape}, y_S {y_S_raw.shape}\")\n",
    "    print(f\"Target (Red Wine) All: X_T_scaled_all {X_T_scaled_all.shape}, y_T {y_T_raw.shape}\")\n",
    "    print(f\"Target Train: X_T_train {X_T_train.shape}, y_T_train {y_T_train.shape} (approx {X_T_train.shape[0]/X_T_raw.shape[0]*100:.1f}%)\")\n",
    "    print(f\"Target Test: X_T_test {X_T_test.shape}, y_T_test {y_T_test.shape}\")\n",
    "\n",
    "    results = {}\n",
    "    default_ls, default_sf, default_noise = 1.0, 1.0, 0.5\n",
    "    default_b, default_mu = 1.0, 1.0 # Initial b and mu for AT-GP\n",
    "\n",
    "    print(\"\\n--- Training AT-GP Model (Wine) ---\")\n",
    "    atgp_model = ATGP(X_S, y_S_raw, X_T_train, y_T_train,\n",
    "                      initial_length_scale=default_ls, initial_sigma_f=default_sf,\n",
    "                      initial_b=default_b, initial_mu=default_mu,\n",
    "                      initial_noise_S_std=default_noise, initial_noise_T_std=default_noise)\n",
    "    atgp_model.fit(disp=verbose_optimization, maxiter=100)\n",
    "    mean_atgp, _ = atgp_model.predict(X_T_test)\n",
    "    results['AT-GP'] = nmse(y_T_test, mean_atgp)\n",
    "    print(f\"AT-GP NMSE (Wine): {results['AT-GP']:.4f}, Params: {atgp_model.optimized_params_}\")\n",
    "\n",
    "    print(\"\\n--- Training No Transfer GP Model (Wine) ---\")\n",
    "    gp_no_transfer = SimpleGPR(initial_length_scale=default_ls, initial_sigma_f=default_sf, initial_noise_std=default_noise)\n",
    "    gp_no_transfer.fit(X_T_train, y_T_train, disp=verbose_optimization, maxiter=100, model_name_for_print=\"No Transfer GP (Wine)\")\n",
    "    mean_no_transfer, _ = gp_no_transfer.predict(X_T_test)\n",
    "    results['No Transfer GP'] = nmse(y_T_test, mean_no_transfer)\n",
    "    print(f\"No Transfer GP NMSE (Wine): {results['No Transfer GP']:.4f}, Params: {gp_no_transfer.optimized_params_}\")\n",
    "\n",
    "    print(\"\\n--- Training Transfer All GP Model (Wine) ---\")\n",
    "    # For Transfer All, source data needs to be on the same scale as target data\n",
    "    X_S_rescaled_for_All = scaler_T_all_wine.transform(scaler_S.inverse_transform(X_S)) # Inverse S-scale, then T-scale\n",
    "    X_all_train = np.vstack((X_S_rescaled_for_All, X_T_train))\n",
    "    y_all_train = np.concatenate((y_S_raw, y_T_train))\n",
    "\n",
    "    gp_transfer_all = SimpleGPR(initial_length_scale=default_ls, initial_sigma_f=default_sf, initial_noise_std=default_noise)\n",
    "    gp_transfer_all.fit(X_all_train, y_all_train, disp=verbose_optimization, maxiter=100, model_name_for_print=\"Transfer All GP (Wine)\")\n",
    "    mean_transfer_all, _ = gp_transfer_all.predict(X_T_test)\n",
    "    results['Transfer All GP'] = nmse(y_T_test, mean_transfer_all)\n",
    "    print(f\"Transfer All GP NMSE (Wine): {results['Transfer All GP']:.4f}, Params: {gp_transfer_all.optimized_params_}\")\n",
    "\n",
    "    print(\"\\n--- Summary of NMSE Results on WINE Dataset ---\")\n",
    "    for model_name, score in results.items():\n",
    "        print(f\"{model_name}: {score:.4f}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_sarcos_experiment(verbose_optimization, seed):\n",
    "    print(\"\\n--- Starting SARCOS Dataset Experiment ---\")\n",
    "    try:\n",
    "        # Use forward slashes or raw strings for paths\n",
    "        input_file_path = 'C:/Disk D/Semester 4 2/Transfer Learning/Assignment/Assignment 3/Own Implementation/sarcos_inv.mat'\n",
    "        output_file_path = 'C:/Disk D/Semester 4 2/Transfer Learning/Assignment/Assignment 3/Own Implementation/sarcos_inv_test.mat'\n",
    "\n",
    "        sarcos_inputs_full = loadmat(input_file_path)['sarcos_inv']  # e.g., (44484, 21)\n",
    "        sarcos_outputs_full = loadmat(output_file_path)['sarcos_inv_test']  # e.g., (4449, 7) - KEY CHANGED HERE as per your update\n",
    "\n",
    "        print(f\"Loaded sarcos_inputs_full: {sarcos_inputs_full.shape}\")\n",
    "        print(f\"Loaded sarcos_outputs_full: {sarcos_outputs_full.shape}\")\n",
    "\n",
    "        # Determine the number of samples to work with (the minimum of the two)\n",
    "        # This assumes the outputs in sarcos_inv_test.mat correspond to the *first* N\n",
    "        # inputs in sarcos_inv.mat, where N is the number of rows in sarcos_inv_test.mat.\n",
    "        num_common_samples = min(sarcos_inputs_full.shape[0], sarcos_outputs_full.shape[0])\n",
    "\n",
    "        if num_common_samples == 0:\n",
    "            print(\"No common samples found or one of the files is empty. Skipping SARCOS.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Working with {num_common_samples} common samples.\")\n",
    "\n",
    "        # Subsample the larger array to match the smaller one\n",
    "        sarcos_inputs_matched = sarcos_inputs_full[:num_common_samples, :]\n",
    "        sarcos_outputs_matched = sarcos_outputs_full[:num_common_samples, :]\n",
    "\n",
    "        if sarcos_outputs_matched.shape[1] < 2: # Ensure at least 2 joints in output\n",
    "            print(f\"SARCOS output data has only {sarcos_outputs_matched.shape[1]} joint(s). Need at least 2. Skipping.\")\n",
    "            return\n",
    "\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"SARCOS .mat files not found. Please download 'sarcos_inv.mat' and 'sarcos_inv_test.mat' (or check paths). Skipping SARCOS experiment.\")\n",
    "        return\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError loading SARCOS data: {e}. Make sure the keys ('sarcos_inv', 'sarcos_inv_test') are correct for your .mat files. Skipping SARCOS experiment.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading SARCOS data: {e}. Skipping SARCOS experiment.\")\n",
    "        return\n",
    "\n",
    "    # --- Subsampling for Source and Target tasks from the matched data ---\n",
    "    # Paper: \"one of the task as the target task and another as the source task\"\n",
    "    # Subsample for demonstration speed. Paper uses all source data.\n",
    "    \n",
    "    # We will select n_source_samples and n_target_total_samples from num_common_samples\n",
    "    n_source_samples_desired = 1000 # Reduced for speed\n",
    "    n_target_total_samples_desired = 1000 # Reduced for speed\n",
    "\n",
    "    if n_source_samples_desired + n_target_total_samples_desired > num_common_samples:\n",
    "        print(f\"Desired samples ({n_source_samples_desired + n_target_total_samples_desired}) exceed available common samples ({num_common_samples}). Adjusting.\")\n",
    "        scale_factor = num_common_samples / (n_source_samples_desired + n_target_total_samples_desired)\n",
    "        n_source_samples = int(n_source_samples_desired * scale_factor)\n",
    "        n_target_total_samples = num_common_samples - n_source_samples\n",
    "        # Ensure a minimum for target to allow splitting\n",
    "        min_target_for_split = 20 # Need at least 2 for train (5% of 20 is 1, need 2)\n",
    "        if n_target_total_samples < min_target_for_split and num_common_samples >= n_source_samples + min_target_for_split :\n",
    "            n_target_total_samples = min_target_for_split\n",
    "            n_source_samples = num_common_samples - n_target_total_samples\n",
    "        elif n_target_total_samples < min_target_for_split: # Not enough even if source is minimal\n",
    "            print(f\"Not enough common samples ({num_common_samples}) to satisfy minimum target samples for split. Skipping.\")\n",
    "            return\n",
    "\n",
    "        if n_source_samples <= 0 : # Ensure some source samples\n",
    "            if num_common_samples > n_target_total_samples:\n",
    "                n_source_samples = num_common_samples - n_target_total_samples\n",
    "            else:\n",
    "                print(\"Cannot allocate source samples. Skipping SARCOS.\")\n",
    "                return\n",
    "        print(f\"Adjusted: n_source_samples={n_source_samples}, n_target_total_samples={n_target_total_samples}\")\n",
    "    else:\n",
    "        n_source_samples = n_source_samples_desired\n",
    "        n_target_total_samples = n_target_total_samples_desired\n",
    "    \n",
    "    if n_source_samples <= 0 or n_target_total_samples <= 0:\n",
    "        print(\"No samples allocated for source or target after adjustment. Skipping SARCOS.\")\n",
    "        return\n",
    "\n",
    "    # Permutation on the common number of samples\n",
    "    indices = np.random.RandomState(seed).permutation(num_common_samples)\n",
    "    \n",
    "    source_indices_from_perm = indices[:n_source_samples]\n",
    "    target_indices_from_perm = indices[n_source_samples : n_source_samples + n_target_total_samples]\n",
    "\n",
    "    # Select data using these valid indices from the matched arrays\n",
    "    X_S_raw = sarcos_inputs_matched[source_indices_from_perm, :]\n",
    "    y_S_raw = sarcos_outputs_matched[source_indices_from_perm, 0] # Joint 1 for source task\n",
    "\n",
    "    X_T_raw_all = sarcos_inputs_matched[target_indices_from_perm, :]\n",
    "    y_T_raw_all = sarcos_outputs_matched[target_indices_from_perm, 1] # Joint 2 for target task\n",
    "\n",
    "    # --- Scaling and Splitting Target Data ---\n",
    "    scaler_S = StandardScaler()\n",
    "    X_S = scaler_S.fit_transform(X_S_raw)\n",
    "\n",
    "    scaler_T_all_sarcos = StandardScaler()\n",
    "    X_T_scaled_all = scaler_T_all_sarcos.fit_transform(X_T_raw_all)\n",
    "\n",
    "    # Use 5% of Target data for training, ensure at least 2 samples for training\n",
    "    # And enough samples for test\n",
    "    if y_T_raw_all.shape[0] < 2: # y_T_raw_all now has n_target_total_samples\n",
    "        print(f\"Not enough samples in y_T_raw_all ({y_T_raw_all.shape[0]}) to split. Skipping SARCOS.\")\n",
    "        return\n",
    "\n",
    "    train_size_target_actual = 0.05\n",
    "    num_train_target = int(y_T_raw_all.shape[0] * train_size_target_actual)\n",
    "\n",
    "    if num_train_target < 2:\n",
    "        if y_T_raw_all.shape[0] >= 2: # If we have at least 2 total, use 2 for training\n",
    "            num_train_target = 2\n",
    "            train_size_target_actual = num_train_target / y_T_raw_all.shape[0]\n",
    "        else: # Should have been caught by y_T_raw_all.shape[0] < 2\n",
    "            print(f\"Cannot get 2 training samples from target data of size {y_T_raw_all.shape[0]}. Skipping SARCOS.\")\n",
    "            return\n",
    "    \n",
    "    if y_T_raw_all.shape[0] - num_train_target < 1: # Ensure at least 1 test sample\n",
    "        print(f\"Not enough samples for testing after allocating {num_train_target} for training from {y_T_raw_all.shape[0]}. Skipping.\")\n",
    "        return\n",
    "\n",
    "\n",
    "    X_T_train, X_T_test, y_T_train, y_T_test = train_test_split(\n",
    "        X_T_scaled_all, y_T_raw_all, train_size=train_size_target_actual, random_state=seed\n",
    "    )\n",
    "    \n",
    "    if X_T_train.shape[0] < 2: # Should be redundant due to above checks\n",
    "        print(\"Not enough target training samples for SARCOS after split. Skipping.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Source (SARCOS Joint 1): X_S {X_S.shape}, y_S {y_S_raw.shape}\")\n",
    "    print(f\"Target Pool (SARCOS Joint 2): X_T_raw_all {X_T_raw_all.shape}, y_T_raw_all {y_T_raw_all.shape}\")\n",
    "    print(f\"Target Train: X_T_train {X_T_train.shape}, y_T_train {y_T_train.shape} (train_size={train_size_target_actual*100:.1f}%)\")\n",
    "    print(f\"Target Test: X_T_test {X_T_test.shape}, y_T_test {y_T_test.shape}\")\n",
    "\n",
    "    results = {}\n",
    "    default_ls, default_sf, default_noise = 1.0, 1.0, 0.1 \n",
    "    default_b, default_mu = 1.0, 1.0\n",
    "    max_iter_sarcos = 30 # Reduced for speed\n",
    "\n",
    "    print(f\"\\n--- Training AT-GP Model (SARCOS, max_iter={max_iter_sarcos}) ---\")\n",
    "    atgp_model = ATGP(X_S, y_S_raw, X_T_train, y_T_train,\n",
    "                      initial_length_scale=default_ls, initial_sigma_f=default_sf,\n",
    "                      initial_b=default_b, initial_mu=default_mu,\n",
    "                      initial_noise_S_std=default_noise, initial_noise_T_std=default_noise)\n",
    "    atgp_model.fit(disp=verbose_optimization, maxiter=max_iter_sarcos)\n",
    "    mean_atgp, _ = atgp_model.predict(X_T_test)\n",
    "    results['AT-GP'] = nmse(y_T_test, mean_atgp)\n",
    "    print(f\"AT-GP NMSE (SARCOS): {results['AT-GP']:.4f}, Params: {atgp_model.optimized_params_}\")\n",
    "\n",
    "    print(f\"\\n--- Training No Transfer GP Model (SARCOS, max_iter={max_iter_sarcos}) ---\")\n",
    "    gp_no_transfer = SimpleGPR(initial_length_scale=default_ls, initial_sigma_f=default_sf, initial_noise_std=default_noise)\n",
    "    gp_no_transfer.fit(X_T_train, y_T_train, disp=verbose_optimization, maxiter=max_iter_sarcos, model_name_for_print=\"No Transfer GP (SARCOS)\")\n",
    "    mean_no_transfer, _ = gp_no_transfer.predict(X_T_test)\n",
    "    results['No Transfer GP'] = nmse(y_T_test, mean_no_transfer)\n",
    "    print(f\"No Transfer GP NMSE (SARCOS): {results['No Transfer GP']:.4f}, Params: {gp_no_transfer.optimized_params_}\")\n",
    "\n",
    "    print(f\"\\n--- Training Transfer All GP Model (SARCOS, max_iter={max_iter_sarcos}) ---\")\n",
    "    X_S_rescaled_for_All = scaler_T_all_sarcos.transform(scaler_S.inverse_transform(X_S))\n",
    "    X_all_train = np.vstack((X_S_rescaled_for_All, X_T_train))\n",
    "    y_all_train = np.concatenate((y_S_raw, y_T_train))\n",
    "\n",
    "    gp_transfer_all = SimpleGPR(initial_length_scale=default_ls, initial_sigma_f=default_sf, initial_noise_std=default_noise)\n",
    "    gp_transfer_all.fit(X_all_train, y_all_train, disp=verbose_optimization, maxiter=max_iter_sarcos, model_name_for_print=\"Transfer All GP (SARCOS)\")\n",
    "    mean_transfer_all, _ = gp_transfer_all.predict(X_T_test)\n",
    "    results['Transfer All GP'] = nmse(y_T_test, mean_transfer_all)\n",
    "    print(f\"Transfer All GP NMSE (SARCOS): {results['Transfer All GP']:.4f}, Params: {gp_transfer_all.optimized_params_}\")\n",
    "\n",
    "    print(\"\\n--- Summary of NMSE Results on SARCOS Dataset ---\")\n",
    "    for model_name, score in results.items():\n",
    "        print(f\"{model_name}: {score:.4f}\")\n",
    "    return results\n",
    "\n",
    "# The rest of your script (wine, sim_wifi, main block) remains the same.\n",
    "# Make sure to replace the run_sarcos_experiment in your main script with this one.\n",
    "\n",
    "def run_simulated_wifi_experiment(verbose_optimization, seed):\n",
    "    print(\"\\n--- Starting SIMULATED WiFi Localization Experiment ---\")\n",
    "    rng = np.random.RandomState(seed)\n",
    "    n_features = 10\n",
    "    n_source_samples = 500\n",
    "    n_target_samples_all = 500 # Total target samples before splitting\n",
    "\n",
    "    # Source task\n",
    "    w_S = rng.randn(n_features, 1)\n",
    "    X_S_raw = rng.rand(n_source_samples, n_features) * 10\n",
    "    y_S_raw = X_S_raw @ w_S + rng.randn(n_source_samples, 1) * 0.5 # Source y\n",
    "\n",
    "    # Target task (related but different)\n",
    "    delta_w = rng.randn(n_features, 1) * 0.5 # Difference in weights\n",
    "    w_T = w_S + delta_w\n",
    "    # Target inputs might have a slight shift too\n",
    "    X_T_raw_all = rng.rand(n_target_samples_all, n_features) * 10 + rng.rand(1, n_features) * 2\n",
    "    y_T_raw_all = X_T_raw_all @ w_T + rng.randn(n_target_samples_all, 1) * 0.5 # Target y\n",
    "\n",
    "    scaler_S = StandardScaler()\n",
    "    X_S = scaler_S.fit_transform(X_S_raw)\n",
    "\n",
    "    scaler_T_all_wifi = StandardScaler()\n",
    "    X_T_scaled_all = scaler_T_all_wifi.fit_transform(X_T_raw_all)\n",
    "\n",
    "    # 5% of target for training\n",
    "    X_T_train, X_T_test, y_T_train, y_T_test = train_test_split(\n",
    "        X_T_scaled_all, y_T_raw_all.ravel(), train_size=0.05, random_state=seed\n",
    "    )\n",
    "    if X_T_train.shape[0] < 2 :\n",
    "        print(\"Not enough target training samples for WiFi Sim after split. Skipping.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Source (Sim WiFi): X_S {X_S.shape}, y_S {y_S_raw.ravel().shape}\")\n",
    "    print(f\"Target (Sim WiFi) All: X_T_scaled_all {X_T_scaled_all.shape}, y_T {y_T_raw_all.ravel().shape}\")\n",
    "    print(f\"Target Train: X_T_train {X_T_train.shape}, y_T_train {y_T_train.shape} (approx {X_T_train.shape[0]/X_T_raw_all.shape[0]*100:.1f}%)\")\n",
    "    print(f\"Target Test: X_T_test {X_T_test.shape}, y_T_test {y_T_test.shape}\")\n",
    "\n",
    "    results = {}\n",
    "    default_ls, default_sf, default_noise = 1.0, 1.0, 0.2\n",
    "    default_b, default_mu = 1.0, 1.0\n",
    "\n",
    "    print(\"\\n--- Training AT-GP Model (Sim WiFi) ---\")\n",
    "    atgp_model = ATGP(X_S, y_S_raw.ravel(), X_T_train, y_T_train,\n",
    "                      initial_length_scale=default_ls, initial_sigma_f=default_sf,\n",
    "                      initial_b=default_b, initial_mu=default_mu,\n",
    "                      initial_noise_S_std=default_noise, initial_noise_T_std=default_noise)\n",
    "    atgp_model.fit(disp=verbose_optimization, maxiter=100)\n",
    "    mean_atgp, _ = atgp_model.predict(X_T_test)\n",
    "    results['AT-GP'] = nmse(y_T_test, mean_atgp)\n",
    "    print(f\"AT-GP NMSE (Sim WiFi): {results['AT-GP']:.4f}, Params: {atgp_model.optimized_params_}\")\n",
    "\n",
    "    print(\"\\n--- Training No Transfer GP Model (Sim WiFi) ---\")\n",
    "    gp_no_transfer = SimpleGPR(initial_length_scale=default_ls, initial_sigma_f=default_sf, initial_noise_std=default_noise)\n",
    "    gp_no_transfer.fit(X_T_train, y_T_train, disp=verbose_optimization, maxiter=100, model_name_for_print=\"No Transfer GP (Sim WiFi)\")\n",
    "    mean_no_transfer, _ = gp_no_transfer.predict(X_T_test)\n",
    "    results['No Transfer GP'] = nmse(y_T_test, mean_no_transfer)\n",
    "    print(f\"No Transfer GP NMSE (Sim WiFi): {results['No Transfer GP']:.4f}, Params: {gp_no_transfer.optimized_params_}\")\n",
    "\n",
    "    print(\"\\n--- Training Transfer All GP Model (Sim WiFi) ---\")\n",
    "    X_S_rescaled_for_All = scaler_T_all_wifi.transform(scaler_S.inverse_transform(X_S))\n",
    "    X_all_train = np.vstack((X_S_rescaled_for_All, X_T_train))\n",
    "    y_all_train = np.concatenate((y_S_raw.ravel(), y_T_train))\n",
    "\n",
    "    gp_transfer_all = SimpleGPR(initial_length_scale=default_ls, initial_sigma_f=default_sf, initial_noise_std=default_noise)\n",
    "    gp_transfer_all.fit(X_all_train, y_all_train, disp=verbose_optimization, maxiter=100, model_name_for_print=\"Transfer All GP (Sim WiFi)\")\n",
    "    mean_transfer_all, _ = gp_transfer_all.predict(X_T_test)\n",
    "    results['Transfer All GP'] = nmse(y_T_test, mean_transfer_all)\n",
    "    print(f\"Transfer All GP NMSE (Sim WiFi): {results['Transfer All GP']:.4f}, Params: {gp_transfer_all.optimized_params_}\")\n",
    "\n",
    "    print(\"\\n--- Summary of NMSE Results on SIMULATED WiFi Dataset ---\")\n",
    "    for model_name, score in results.items():\n",
    "        print(f\"{model_name}: {score:.4f}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting SIMULATED WiFi Localization Experiment ---\n",
      "Source (Sim WiFi): X_S (500, 10), y_S (500,)\n",
      "Target (Sim WiFi) All: X_T_scaled_all (500, 10), y_T (500,)\n",
      "Target Train: X_T_train (25, 10), y_T_train (25,) (approx 5.0%)\n",
      "Target Test: X_T_test (475, 10), y_T_test (475,)\n",
      "\n",
      "--- Training AT-GP Model (Sim WiFi) ---\n",
      "AT-GP NMSE (Sim WiFi): 0.0039, Params: {'ls': np.float64(192.1643), 'sf': np.float64(601.5818), 'b': np.float64(9.601), 'mu': np.float64(0.0), 'noise_S': np.float64(43.7325), 'noise_T': np.float64(0.3646), 'lambda': np.float64(0.9997)}\n",
      "\n",
      "--- Training No Transfer GP Model (Sim WiFi) ---\n",
      "No Transfer GP NMSE (Sim WiFi): 0.0040, Params: {'ls': np.float64(210.1262), 'sf': np.float64(366.5622), 'noise': np.float64(0.387)}\n",
      "\n",
      "--- Training Transfer All GP Model (Sim WiFi) ---\n",
      "Transfer All GP NMSE (Sim WiFi): 1.0114, Params: {'ls': np.float64(76.2268), 'sf': np.float64(162.6419), 'noise': np.float64(2.201)}\n",
      "\n",
      "--- Summary of NMSE Results on SIMULATED WiFi Dataset ---\n",
      "AT-GP: 0.0039\n",
      "No Transfer GP: 0.0040\n",
      "Transfer All GP: 1.0114\n",
      "\n",
      "\n",
      "--- OVERALL RESULTS SUMMARY ---\n",
      "\n",
      "Simulated WiFi Dataset:\n",
      "  AT-GP: 0.0039\n",
      "  No Transfer GP: 0.0040\n",
      "  Transfer All GP: 1.0114\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    np.set_printoptions(precision=4, suppress=True)\n",
    "    np.seterr(all='ignore') # Suppress common warnings during optimization\n",
    "    verbose_opt = False  # Set to True for detailed optimization output from L-BFGS-B\n",
    "    random_seed = 42 # For reproducibility\n",
    "\n",
    "    sim_wifi_results = run_simulated_wifi_experiment(verbose_optimization=verbose_opt, seed=random_seed)\n",
    "\n",
    "    print(\"\\n\\n--- OVERALL RESULTS SUMMARY ---\")\n",
    "    if sim_wifi_results:\n",
    "        print(\"\\nSimulated WiFi Dataset:\")\n",
    "        for model, nmse_val in sim_wifi_results.items(): print(f\"  {model}: {nmse_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting SARCOS Dataset Experiment ---\n",
      "Loaded sarcos_inputs_full: (44484, 28)\n",
      "Loaded sarcos_outputs_full: (4449, 28)\n",
      "Working with 4449 common samples.\n",
      "Source (SARCOS Joint 1): X_S (1000, 28), y_S (1000,)\n",
      "Target Pool (SARCOS Joint 2): X_T_raw_all (1000, 28), y_T_raw_all (1000,)\n",
      "Target Train: X_T_train (50, 28), y_T_train (50,) (train_size=5.0%)\n",
      "Target Test: X_T_test (950, 28), y_T_test (950,)\n",
      "\n",
      "--- Training AT-GP Model (SARCOS, max_iter=30) ---\n",
      "AT-GP NMSE (SARCOS): 1.0150, Params: {'ls': np.float64(118.1881), 'sf': np.float64(0.4545), 'b': np.float64(0.0), 'mu': np.float64(0.0), 'noise_S': np.float64(6.0924), 'noise_T': np.float64(0.1072), 'lambda': np.float64(1.0)}\n",
      "\n",
      "--- Training No Transfer GP Model (SARCOS, max_iter=30) ---\n",
      "No Transfer GP NMSE (SARCOS): 1.0206, Params: {'ls': np.float64(688.068), 'sf': np.float64(0.335), 'noise': np.float64(0.1084)}\n",
      "\n",
      "--- Training Transfer All GP Model (SARCOS, max_iter=30) ---\n",
      "Transfer All GP NMSE (SARCOS): 4.4362, Params: {'ls': np.float64(1.5076), 'sf': np.float64(0.2815), 'noise': np.float64(0.204)}\n",
      "\n",
      "--- Summary of NMSE Results on SARCOS Dataset ---\n",
      "AT-GP: 1.0150\n",
      "No Transfer GP: 1.0206\n",
      "Transfer All GP: 4.4362\n",
      "\n",
      "\n",
      "--- OVERALL RESULTS SUMMARY ---\n",
      "\n",
      "SARCOS Dataset (Subsampled, J1->J2):\n",
      "  AT-GP: 1.0150\n",
      "  No Transfer GP: 1.0206\n",
      "  Transfer All GP: 4.4362\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    np.set_printoptions(precision=4, suppress=True)\n",
    "    np.seterr(all='ignore') # Suppress common warnings during optimization\n",
    "    verbose_opt = False  # Set to True for detailed optimization output from L-BFGS-B\n",
    "    random_seed = 42 # For reproducibility\n",
    "\n",
    "    # Run experiments for each dataset\n",
    "    sarcos_results = run_sarcos_experiment(verbose_optimization=verbose_opt, seed=random_seed)\n",
    "    print(\"\\n\\n--- OVERALL RESULTS SUMMARY ---\")\n",
    "    if sarcos_results:\n",
    "        print(\"\\nSARCOS Dataset (Subsampled, J1->J2):\")\n",
    "        for model, nmse_val in sarcos_results.items(): print(f\"  {model}: {nmse_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting WINE Dataset Experiment ---\n",
      "Source (White Wine): X_S (4898, 11), y_S (4898,)\n",
      "Target (Red Wine) All: X_T_scaled_all (1599, 11), y_T (1599,)\n",
      "Target Train: X_T_train (79, 11), y_T_train (79,) (approx 4.9%)\n",
      "Target Test: X_T_test (1520, 11), y_T_test (1520,)\n",
      "\n",
      "--- Training AT-GP Model (Wine) ---\n",
      "AT-GP NMSE (Wine): 0.7345, Params: {'ls': np.float64(102.3041), 'sf': np.float64(103.0598), 'b': np.float64(0.0003), 'mu': np.float64(3.0226), 'noise_S': np.float64(6.9137), 'noise_T': np.float64(0.5119), 'lambda': np.float64(0.9992)}\n",
      "\n",
      "--- Training No Transfer GP Model (Wine) ---\n",
      "No Transfer GP NMSE (Wine): 0.7379, Params: {'ls': np.float64(379.5052), 'sf': np.float64(58.9384), 'noise': np.float64(0.6133)}\n",
      "\n",
      "--- Training Transfer All GP Model (Wine) ---\n",
      "Transfer All GP NMSE (Wine): 1.0631, Params: {'ls': np.float64(38.023), 'sf': np.float64(37.9402), 'noise': np.float64(0.7059)}\n",
      "\n",
      "--- Summary of NMSE Results on WINE Dataset ---\n",
      "AT-GP: 0.7345\n",
      "No Transfer GP: 0.7379\n",
      "Transfer All GP: 1.0631\n",
      "\n",
      "\n",
      "--- OVERALL RESULTS SUMMARY ---\n",
      "\n",
      "Simulated WiFi Dataset:\n",
      "  AT-GP: 0.7345\n",
      "  No Transfer GP: 0.7379\n",
      "  Transfer All GP: 1.0631\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    np.set_printoptions(precision=4, suppress=True)\n",
    "    np.seterr(all='ignore') # Suppress common warnings during optimization\n",
    "    verbose_opt = False  # Set to True for detailed optimization output from L-BFGS-B\n",
    "    random_seed = 42 # For reproducibility\n",
    "\n",
    "    sim_wine_results = run_wine_experiment(verbose_optimization=verbose_opt, seed=random_seed)\n",
    "\n",
    "    print(\"\\n\\n--- OVERALL RESULTS SUMMARY ---\")\n",
    "    if sim_wine_results:\n",
    "        print(\"\\nSimulated Wine Dataset:\")\n",
    "        for model, nmse_val in sim_wine_results.items(): print(f\"  {model}: {nmse_val:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
